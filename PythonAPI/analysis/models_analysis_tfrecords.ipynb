{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"4\"\n",
    "\n",
    "from scipy.special import entr # see https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.entr.html\n",
    "from keras.utils import to_categorical\n",
    "import keras.metrics as metrics\n",
    "from kf_impl import EKF_CV_MODEL\n",
    "from lstm_impl import CombinedLSTM\n",
    "from cnn_lstm_impl import CombinedCNNLSTM\n",
    "import pdb\n",
    "from utils import extract_data, sup_plot\n",
    "import random\n",
    "from tfrecord_utils import  read_tfrecord\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_train_test_splits(tf_files_list, num_tf_folds=5):\n",
    "#     # Using a suboptimal approach here:\n",
    "#     # Just build a list of dictionaries, where entry_i \n",
    "#     # corresponds to train split i.\n",
    "\n",
    "#     train_sets = []\n",
    "#     test_sets  = []\n",
    "#     folds  = []\n",
    "    \n",
    "#     inds = np.arange(num_tf_folds)\n",
    "    \n",
    "#     random.seed(0)\n",
    "#     random.shuffle(tf_files_list)\n",
    "#     n_files = len(tf_files_list)\n",
    "    \n",
    "    \n",
    "#     splits = (n_files // num_tf_folds) * np.ones(num_tf_folds)\n",
    "#     splits[:n_files % num_tf_folds] += 1\n",
    "    \n",
    "#     ind_limits = np.cumsum(splits).astype(np.int)\n",
    "    \n",
    "#     for i in range(len(ind_limits)):\n",
    "#         if i == 0:\n",
    "#             ind_start = 0\n",
    "#         else:\n",
    "#             ind_start = ind_limits[i-1]\n",
    "#         ind_end = ind_limits[i]\n",
    "        \n",
    "#         folds.append(tf_files_list[ind_start:ind_end])\n",
    "\n",
    "#     flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#     for hold_out_ind in inds:\n",
    "#         train_inds = np.delete(inds, hold_out_ind)\n",
    "#         test_inds = [hold_out_ind]\n",
    "        \n",
    "#         train_sets.append(flatten([y for y in [folds[x] for x in train_inds]]))\n",
    "#         test_sets.append(flatten([y for y in [folds[x] for x in test_inds]]))\n",
    "                \n",
    "#     return train_sets, test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_splits(tf_files_list, num_tf_folds=5):\n",
    "    # Using a suboptimal approach here:\n",
    "    # Just build a list of dictionaries, where entry_i \n",
    "    # corresponds to train split i.\n",
    "\n",
    "    train_sets = []\n",
    "    test_sets  = []\n",
    "     \n",
    "    for test_fold_ind in range(num_tf_folds):\n",
    "        train_set = [x for x in tf_files_list if int(x.split('_')[2]) != test_fold_ind]\n",
    "        test_set = [x for x in tf_files_list if int(x.split('_')[2]) == test_fold_ind]\n",
    "        \n",
    "        train_sets.append(train_set)\n",
    "        test_sets.append(test_set)\n",
    "        \n",
    "    return train_sets, test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_dist_by_timestep(goal_pred, traj_pred_dict, traj_actual):\n",
    "    # M = # of instances, N = time horizon, 2 (xy) \n",
    "    M = traj_pred_dict[0].shape[0]\n",
    "    N = traj_pred_dict[0].shape[1]\n",
    "    \n",
    "    weighted_sum = np.zeros((M, N))\n",
    "    num_pred_traj = len(traj_pred_dict.keys())\n",
    "    top_k_probs = -np.sort(-goal_pred, axis=1)[:,:num_pred_traj]\n",
    "    \n",
    "    for k in range(num_pred_traj):\n",
    "        # key = 0 \n",
    "        traj_pred_k = traj_pred_dict[k] # M by N by 2\n",
    "        diff = traj_pred_k - traj_actual # M by N by 2\n",
    "        diff_xy_norm = np.linalg.norm(diff, axis=2) # M by N\n",
    "\n",
    "        for i in range(N):\n",
    "            diff_xy_norm[:,i] *= top_k_probs[:,k]\n",
    "        \n",
    "        weighted_sum += diff_xy_norm\n",
    "    return np.mean(weighted_sum, axis=0)\n",
    "\n",
    "def dist_by_timestep(traj_pred_dict, traj_actual):\n",
    "    # returns avg, min, max distance error across each timestep\n",
    "    diff = traj_pred_dict[0] - traj_actual # N by N_pred by 2\n",
    "    diff_xy_norm = np.linalg.norm(diff, axis=2)\n",
    "    return np.mean(diff_xy_norm, axis=0), np.min(diff_xy_norm, axis = 0), np.max(diff_xy_norm, axis=0)\n",
    "\n",
    "def top_k_accuracy(goal_pred, goal_actual, k=1):\n",
    "    # returns empirical probability of the real goal being contained\n",
    "    # in the top k most likely goal set from goal_pred.\n",
    "    return np.mean(metrics.top_k_categorical_accuracy(goal_actual, goal_pred, k=k))\n",
    "\n",
    "def mean_entropy(goal_pred):\n",
    "    # returns the avg. entropy of the goal prediction dist.\n",
    "    # higher entropy indicates more uncertain predictions\n",
    "    N = goal_pred.shape[0]\n",
    "    \n",
    "    entr_matrix = entr(goal_pred)\n",
    "    entr_by_instance = np.sum(entr_matrix, axis=1) #entropy by snippet\n",
    "    return np.mean(entr_by_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Construct the evaluation   datasets.\n",
    "MODE = 'TRAIN' # 'TRAIN' or 'LOAD'\n",
    "\n",
    "tffiles_to_process = glob.glob('../examples/bags/*.tfrecord')\n",
    "train_sets, test_sets = build_train_test_splits(tffiles_to_process, num_tf_folds=5)\n",
    "history_shape       = (None, 5, 3)\n",
    "goal_position_shape = (None, 32*3)\n",
    "one_hot_goal_shape  = (None, 32+1)\n",
    "future_shape        = (None, 20, 2)\n",
    "image_input_shape = (5,650,200,3)\n",
    "hidden_dim = 100\n",
    "top_k_goal = [0,1,2]\n",
    "\n",
    "model = CombinedLSTM(history_shape,\n",
    "                 goal_position_shape,\n",
    "                 image_input_shape,\n",
    "                 one_hot_goal_shape,\n",
    "                 future_shape,\n",
    "                 hidden_dim,\n",
    "                 beta=1,\n",
    "                 gamma=1,\n",
    "                 use_goal_info=False)\n",
    "#model.fit(train_sets[0], test_sets[0],verbose=1)\n",
    "\n",
    "goal_pred, goal_gt, traj_pred_dict, traj_gt = model.predict(test_sets[1])\n",
    "print(goal_pred.shape,goal_gt.shape,traj_pred_dict[0].shape,traj_gt.shape)\n",
    "#print(np.sum(goal_gt,axis=0))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EKF_CV, Fold 0 at  1579392300.4291968\n",
      "skipping lstm\n",
      "Training EKF_CV, Fold 1 at  1579392300.4293509\n",
      "skipping lstm\n",
      "Training EKF_CV, Fold 2 at  1579392300.4293945\n",
      "skipping lstm\n",
      "Training EKF_CV, Fold 3 at  1579392300.4305348\n",
      "skipping lstm\n",
      "Training EKF_CV, Fold 4 at  1579392300.4305787\n",
      "skipping lstm\n",
      "Finished model:  EKF_CV\n",
      "Training LSTM_b0.100_g1.000, Fold 0 at  1579392300.4306545\n",
      "Skipping lstm\n",
      "Training LSTM_b0.100_g1.000, Fold 1 at  1579392300.4317744\n",
      "Skipping lstm\n",
      "Training LSTM_b0.100_g1.000, Fold 2 at  1579392300.4321368\n",
      "Skipping lstm\n",
      "Training LSTM_b0.100_g1.000, Fold 3 at  1579392300.4324188\n",
      "Skipping lstm\n",
      "Training LSTM_b0.100_g1.000, Fold 4 at  1579392300.4327629\n",
      "Skipping lstm\n",
      "Finished model:  LSTM_b0.100_g1.000\n",
      "Training LSTM_b0.500_g1.000, Fold 0 at  1579392300.4328387\n",
      "Skipping lstm\n",
      "Training LSTM_b0.500_g1.000, Fold 1 at  1579392300.432881\n",
      "Skipping lstm\n",
      "Training LSTM_b0.500_g1.000, Fold 2 at  1579392300.4340236\n",
      "Skipping lstm\n",
      "Training LSTM_b0.500_g1.000, Fold 3 at  1579392300.4340723\n",
      "Skipping lstm\n",
      "Training LSTM_b0.500_g1.000, Fold 4 at  1579392300.4341145\n",
      "Skipping lstm\n",
      "Finished model:  LSTM_b0.500_g1.000\n",
      "Training LSTM_b1.000_g1.000, Fold 0 at  1579392300.435025\n",
      "Skipping lstm\n",
      "Training LSTM_b1.000_g1.000, Fold 1 at  1579392300.4350731\n",
      "Skipping lstm\n",
      "Training LSTM_b1.000_g1.000, Fold 2 at  1579392300.4358633\n",
      "Skipping lstm\n",
      "Training LSTM_b1.000_g1.000, Fold 3 at  1579392300.4360497\n",
      "Skipping lstm\n",
      "Training LSTM_b1.000_g1.000, Fold 4 at  1579392300.4360938\n",
      "Skipping lstm\n",
      "Finished model:  LSTM_b1.000_g1.000\n",
      "Training LSTM_no_goal, Fold 0 at  1579392300.436166\n",
      "Skipping lstm\n",
      "Training LSTM_no_goal, Fold 1 at  1579392300.4362085\n",
      "Skipping lstm\n",
      "Training LSTM_no_goal, Fold 2 at  1579392300.436252\n",
      "Skipping lstm\n",
      "Training LSTM_no_goal, Fold 3 at  1579392300.436309\n",
      "Skipping lstm\n",
      "Training LSTM_no_goal, Fold 4 at  1579392300.4363518\n",
      "Skipping lstm\n",
      "Finished model:  LSTM_no_goal\n",
      "Training CNN_b1.000_g1.000, Fold 0 at  1579392300.4364264\n",
      "CNN goal  4.997470855712891\n",
      "CNN goal  1.5036523342132568\n",
      "CNN goal  1.5725829601287842\n",
      "CNN goal  1.6012096405029297\n",
      "CNN goal  1.517953634262085\n",
      "CNN traj  4.7040205001831055\n",
      "CNN traj  2.2928411960601807\n",
      "CNN traj  2.242173433303833\n",
      "CNN traj  2.2606308460235596\n",
      "CNN traj  2.2958168983459473\n",
      "Training CNN_b1.000_g1.000, Fold 1 at  1579392325.8402514\n",
      "CNN goal  1.5813751220703125\n",
      "CNN goal  1.6517856121063232\n",
      "CNN goal  1.6793420314788818\n",
      "CNN goal  1.5394291877746582\n",
      "CNN goal  1.6938493251800537\n",
      "CNN traj  2.2941293716430664\n",
      "CNN traj  2.2766222953796387\n",
      "CNN traj  2.2065916061401367\n",
      "CNN traj  2.3412585258483887\n",
      "CNN traj  2.365997552871704\n",
      "Training CNN_b1.000_g1.000, Fold 2 at  1579392345.536307\n",
      "CNN goal  1.622105360031128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5e1ee174eb57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;31m#model.fit(train_set, test_set, num_epochs=100, batch_size=32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m'CNN'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipping lstm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/govvijay/carla/PythonAPI/analysis/cnn_lstm_impl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_set, val_set, num_epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \t\tself.goal_model.fit_model(train_set, val_set, num_epochs=num_epochs, batch_size=batch_size, \\\n\u001b[0;32m---> 58\u001b[0;31m \t\t                          verbose=verbose)\n\u001b[0m\u001b[1;32m     59\u001b[0m \t\tself.traj_model.fit_model(train_set, val_set, num_epochs=num_epochs, batch_size=batch_size, \\\n\u001b[1;32m     60\u001b[0m \t\t\t                      verbose=verbose)\n",
      "\u001b[0;32m/shared/govvijay/carla/PythonAPI/analysis/cnn_lstm_impl.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(self, train_set, val_set, num_epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carla_analysis/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carla_analysis/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carla_analysis/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carla_analysis/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2660\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Construct the evaluation datasets.\n",
    "MODE = 'TRAIN' # 'TRAIN' or 'LOAD'\n",
    "res_filename = 'model_comparison_dict.pkl'\n",
    "\n",
    "tffiles_to_process = glob.glob('../examples/bags/dataset*.tfrecord')\n",
    "train_sets, test_sets = build_train_test_splits(tffiles_to_process, num_tf_folds=5)\n",
    "\n",
    "# Create saving directories.\n",
    "if not os.path.exists('./model'):\n",
    "    os.makedirs('./model')\n",
    "if not os.path.exists('./results'):\n",
    "    os.makedirs('./results')\n",
    "\n",
    "# Build the model bank.\n",
    "models = []\n",
    "names = []\n",
    "\n",
    "# Build Trajectory Model\n",
    "# history_shape = train_sets[0]['history_traj_data'].shape\n",
    "# goal_position_shape = train_sets[0]['goal_position'].shape\n",
    "# one_hot_goal_shape = train_sets[0]['one_hot_goal'].shape\n",
    "# future_shape = train_sets[0]['future_traj_data'].shape\n",
    "# Hard coded for now, need to make this more robust:\n",
    "history_shape       = (None, 5, 3)\n",
    "goal_position_shape = (None, 32*3)\n",
    "one_hot_goal_shape  = (None, 32+1)\n",
    "future_shape        = (None, 20, 2)\n",
    "image_input_shape = (5,325,100,3)\n",
    "hidden_dim = 100\n",
    "top_k_goal = [0,1,2]\n",
    "\n",
    "# Baseline\n",
    "models.append(EKF_CV_MODEL(x_init=np.zeros(5), P_init=np.eye(5), R=np.diag([1e-3]*3), dt=0.1))\n",
    "names.append('EKF_CV')\n",
    "\n",
    "# for gamma in [0.001, 0.1, 1.0]:\n",
    "#      for beta in [0.001, 0.1, 1.0]:\n",
    "for beta in [0.1, 0.5, 1.0]:\n",
    "    for gamma in [1.0]:\n",
    "        models.append(\n",
    "                CombinedLSTM(history_shape,\n",
    "                             goal_position_shape,\n",
    "                             one_hot_goal_shape,\n",
    "                             future_shape,\n",
    "                             hidden_dim,\n",
    "                             beta=beta,\n",
    "                             gamma=gamma,\n",
    "                             use_goal_info=True)\n",
    "            )\n",
    "\n",
    "        names.append('LSTM_b%.3f_g%.3f' % (beta, gamma)) # ground truth goal, anyone can be used for traj LSTM\n",
    "\n",
    "models.append(\n",
    "    CombinedLSTM(history_shape,\n",
    "                 goal_position_shape,\n",
    "                 one_hot_goal_shape,\n",
    "                 future_shape,\n",
    "                 hidden_dim,\n",
    "                 beta=beta,\n",
    "                 gamma=gamma,\n",
    "                 use_goal_info=False))\n",
    "names.append('LSTM_no_goal')\n",
    "\n",
    "for use_goal in [True, False]:\n",
    "    models.append(\n",
    "        CombinedCNNLSTM(history_shape,\n",
    "                     goal_position_shape,\n",
    "                     image_input_shape,\n",
    "                     one_hot_goal_shape,\n",
    "                     future_shape,\n",
    "                     hidden_dim,\n",
    "                     beta=beta,\n",
    "                     gamma=gamma,\n",
    "                     use_goal_info=use_goal))\n",
    "    names.append('CNN_b%.3f_g%.3f' % (beta,gamma) if use_goal  \\\n",
    "                 else 'CNN_b%.3f_g%.3f_no_goal' %(beta,gamma))\n",
    "        \n",
    "model_res_dict = {} # same indexing/length as names/models\n",
    "if MODE is 'TRAIN':\n",
    "    for name, model in zip(names, models):\n",
    "        metric_dict = {}\n",
    "        metric_dict['train'] = {'N_instances'   : [],\n",
    "                                'traj_dist_vs_N': [],   # no goal\n",
    "                                'wtraj_dist_vs_N': [],  # weighted, multimodal\n",
    "                                'gtraj_dist_vs_N': [],  # gt\n",
    "                                'goal_top_1_acc': [], \n",
    "                                'goal_top_3_acc': [],\n",
    "                                'goal_top_5_acc': [],\n",
    "                                'goal_entropy'  : []}\n",
    "        metric_dict['test']  = {'N_instances'   : [],\n",
    "                                'traj_dist_vs_N': [],   # no goal\n",
    "                                'wtraj_dist_vs_N': [],  # weighted, multimodal\n",
    "                                'gtraj_dist_vs_N': [],  # gt\n",
    "                                'goal_top_1_acc': [], \n",
    "                                'goal_top_3_acc': [],\n",
    "                                'goal_top_5_acc': [],\n",
    "                                'goal_entropy'  : []}\n",
    "\n",
    "        for i_fold, (train_set, test_set) in enumerate(zip(train_sets, test_sets)):\n",
    "            \n",
    "            print('Training %s, Fold %d at ' % (name, i_fold), time.time())\n",
    "            if 'LSTM' in name:\n",
    "                print('Skipping lstm')\n",
    "                #model.fit(train_set, test_set, num_epochs=100, batch_size=32)\n",
    "            elif 'CNN' in name:\n",
    "                model.fit(train_set, test_set, num_epochs=5, batch_size=32)\n",
    "            else:\n",
    "                print('skipping lstm')\n",
    "                #model.fit(train_set, test_set)\n",
    "            #model.save('./model/%s_fold%d' % (name, i_fold))\n",
    "            continue\n",
    "            for tkey, tset in zip(['train', 'test'], [train_set, test_set]):\n",
    "                print('\\tStarted prediction for tkey: ', tkey, ' at ', time.time())\n",
    "                goal_pred, goal_gt, traj_pred_dict, traj_gt = model.predict(tset) # either no goal or ground truth\n",
    "                N_instances = goal_pred.shape[0]\n",
    "                \n",
    "                if 'no_goal' in name or 'EKF_CV' in name:\n",
    "                    # just populate the traj_dist_vs_N\n",
    "                    meand, mind, maxd = dist_by_timestep(traj_pred_dict, traj_gt[:,:,:2])\n",
    "                    metric_dict[tkey]['traj_dist_vs_N'].append(meand)\n",
    "                    \n",
    "                    if 'LSTM' in name:\n",
    "                        print('was the goal used?: ', model.traj_model.use_goal_info)\n",
    "                else:\n",
    "                    # ugly hack: every multimodal case can keep the same gt result.\n",
    "                    # just look up one multimodal case for plotting.\n",
    "                    meand, mind, maxd = dist_by_timestep(traj_pred_dict, traj_gt[:,:,:2])\n",
    "                    metric_dict[tkey]['gtraj_dist_vs_N'].append(meand)\n",
    "                    \n",
    "                    # multimodal predictions with weighted distance by timestep.\n",
    "                    goal_pred, goal_gt, traj_pred_dict, traj_gt = model.predict(tset, top_k_goal)\n",
    "                    wmeand = weighted_dist_by_timestep(goal_pred, traj_pred_dict, traj_gt[:,:,:2])\n",
    "                    metric_dict[tkey]['wtraj_dist_vs_N'].append(wmeand)\n",
    "                \n",
    "                \n",
    "                \n",
    "                t1 = top_k_accuracy(goal_pred, goal_gt, k=1)\n",
    "                t3 = top_k_accuracy(goal_pred, goal_gt, k=3)\n",
    "                t5 = top_k_accuracy(goal_pred, goal_gt, k=5)\n",
    "\n",
    "                ment = mean_entropy(goal_pred)\n",
    "                metric_dict[tkey]['N_instances'].append(N_instances)\n",
    "                metric_dict[tkey]['goal_top_1_acc'].append(t1)\n",
    "                metric_dict[tkey]['goal_top_3_acc'].append(t3)\n",
    "                metric_dict[tkey]['goal_top_5_acc'].append(t5)\n",
    "                metric_dict[tkey]['goal_entropy'].append(ment)\n",
    "\n",
    "                # TODO: save predictions/labels?\n",
    "\n",
    "        model_res_dict[name] = metric_dict\n",
    "        print('Finished model: ', name)\n",
    "\n",
    "    # save the model_res_dict to ./results\n",
    "    pickle.dump(model_res_dict, open('./results/%s' % res_filename, 'wb'))\n",
    "elif MODE is 'LOAD':\n",
    "    # TODO: maybe load models if needed?\n",
    "    model_res_dict = pickle.load(open('./results/%s' % res_filename, 'rb'))\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "\n",
    "# for record in train_sets[0]:\n",
    "#     p = record.split('_')[1][1]\n",
    "#     t = record.split('_')[2][1]\n",
    "#     e = record.split('_')[3][1]\n",
    "    \n",
    "#     data_list.append(['train', p, t, e])\n",
    "\n",
    "# for record in test_sets[0]:\n",
    "#     p = record.split('_')[1][1]\n",
    "#     t = record.split('_')[2][1]\n",
    "#     e = record.split('_')[3][1]\n",
    "    \n",
    "#     data_list.append(['test', p, t, e])\n",
    "    \n",
    "# traj_df = pd.DataFrame(data_list, columns=['Split', 'PID', 'TID', 'EID'],dtype=float)\n",
    "# print( traj_df[(traj_df.Split == 'train') & (traj_df.PID == 1)].count() )\n",
    "# print( traj_df[(traj_df.Split == 'test') & (traj_df.PID == 1)].count())\n",
    "\n",
    "# print( traj_df[(traj_df.Split == 'train') & (traj_df.PID == 2)].count())\n",
    "# print( traj_df[(traj_df.Split == 'test') & (traj_df.PID == 2)].count())\n",
    "\n",
    "# print( traj_df[(traj_df.Split == 'train') & (traj_df.PID == 3)].count())\n",
    "# print( traj_df[(traj_df.Split == 'test') & (traj_df.PID == 3)].count())\n",
    "\n",
    "# for i in range(9):\n",
    "#     print('Episode ', i)\n",
    "#     print( traj_df[(traj_df.Split == 'train') & (traj_df.EID == i)].count())\n",
    "#     print( traj_df[(traj_df.Split == 'test') & (traj_df.EID == i)].count())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: visualize the snippets.  Needs to be done with tfrecord.\n",
    "'''\n",
    "# Get the data\n",
    "pklfiles_to_process = glob.glob('./dataset/*.pkl')\n",
    "pklfiles_to_process.sort()\n",
    "print('Found %d pkl files: %s' % (len(pklfiles_to_process), pklfiles_to_process))\n",
    "\n",
    "file_num = 0\n",
    "\n",
    "pklfile = pklfiles_to_process[file_num]\n",
    "\n",
    "vtest_set  = {\"history_traj_data\" : None,\n",
    "             \"future_traj_data\"  : None,\n",
    "             \"goal_position\"     : None,\n",
    "             \"one_hot_goal\"      : None}\n",
    "vtest_set_kf  = {\"history_traj_data\" : None,\n",
    "             \"future_traj_data\"  : None,\n",
    "             \"goal_position\"     : None,\n",
    "             \"one_hot_goal\"      : None}\n",
    "\n",
    "vtest_set['history_traj_data'], vtest_set['future_traj_data'], vtest_set['goal_position'], vtest_set['one_hot_goal'], traj_idx = extract_data(pklfile, full_traj=True, crop_traj=True)\n",
    "vtest_set_kf['history_traj_data'], vtest_set_kf['future_traj_data'], vtest_set_kf['goal_position'], vtest_set_kf['one_hot_goal'], traj_idx_kf = extract_data(pklfile, full_traj=True, crop_traj=False)\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    if 'EKF_CV' in name:\n",
    "        goal_pred, traj_pred_dict = model.predict(vtest_set_kf)\n",
    "    elif 'no_goal' in name:\n",
    "        continue\n",
    "    else:\n",
    "        goal_pred, traj_pred_dict = model.predict(vtest_set, top_k_goal=top_k_goal)\n",
    "        \n",
    "    sup_plot(name, vtest_set, traj_idx, goal_pred, traj_pred_dict, limit=2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLOT 1: timestep vs. mean distance error\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_list = []\n",
    "timesteps = np.arange(20) # TODO: hard coded for now, change later.\n",
    "\n",
    "for model in model_res_dict.keys():\n",
    "    for split in model_res_dict[model].keys(): \n",
    "        # train/test\n",
    "        if 'no_goal' in model:\n",
    "            name = model\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['traj_dist_vs_N']\n",
    "        elif 'EKF' in model:\n",
    "            name = model\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['traj_dist_vs_N']\n",
    "            # hack to only get ground truth goal based traj. pred once\n",
    "        elif 'b0.100_g1.000' in model:\n",
    "            name = 'LSTM_gt_goal'\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['gtraj_dist_vs_N']\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        for i_fold, td in enumerate(traj_dist_vs_N):\n",
    "            for j_timestep, dist_timestep in enumerate(td):\n",
    "                data_list.append([name, split, i_fold, j_timestep, dist_timestep])\n",
    "            \n",
    "traj_df = pd.DataFrame(data_list, columns=['Model', 'Split', 'Fold', 'Timestep', 'Distance Error'],dtype=float)\n",
    "print(traj_df)\n",
    "\n",
    "print('TRAIN')\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Distance Error\", hue=\"Model\", data=traj_df[traj_df.Split == 'train'])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()\n",
    "\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Distance Error\", hue=\"Model\", data=traj_df[(traj_df.Split == 'train') & \\\n",
    "                                                                              (traj_df.Model != 'EKF_CV')])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()\n",
    "\n",
    "print('TEST')\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Distance Error\", hue=\"Model\", data=traj_df[traj_df.Split == 'test'])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()\n",
    "\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Distance Error\", hue=\"Model\", data=traj_df[(traj_df.Split == 'test') & \\\n",
    "                                                                              (traj_df.Model != 'EKF_CV')])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 2: timestep vs. weighted mean distance error\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_list = []\n",
    "timesteps = np.arange(20) # TODO: hard coded for now, change later.\n",
    "\n",
    "for model in model_res_dict.keys():\n",
    "    for split in model_res_dict[model].keys(): \n",
    "        # train/test\n",
    "        if 'no_goal' in model:\n",
    "            name = model\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['traj_dist_vs_N']\n",
    "        elif 'EKF' in model:\n",
    "            continue\n",
    "        else:\n",
    "            name = model\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['wtraj_dist_vs_N']  \n",
    "            \n",
    "        for i_fold, td in enumerate(traj_dist_vs_N):\n",
    "            for j_timestep, dist_timestep in enumerate(td):\n",
    "                data_list.append([name, split, i_fold, j_timestep, dist_timestep])\n",
    "\n",
    "# gtraj_dist_vs_N_test = model_res_dict['LSTM_b0.100_g0.100']['test'] ['gtraj_dist_vs_N'] \n",
    "# for i_fold, td in enumerate(gtraj_dist_vs_N_test):\n",
    "#     for j_timestep, dist_timestep in enumerate(td):\n",
    "#         data_list.append(['LSTM_gt_goal', 'test', i_fold, j_timestep, dist_timestep])\n",
    "\n",
    "# gtraj_dist_vs_N_train = model_res_dict['LSTM_b0.100_g0.001']['train']['gtraj_dist_vs_N'] \n",
    "# for i_fold, td in enumerate(gtraj_dist_vs_N_train):\n",
    "#     for j_timestep, dist_timestep in enumerate(td):\n",
    "#         data_list.append(['LSTM_gt_goal', 'train', i_fold, j_timestep, dist_timestep])\n",
    "                \n",
    "traj_df = pd.DataFrame(data_list, columns=['Model', 'Split', 'Fold', 'Timestep', 'Weighted Distance Error'],dtype=float)\n",
    "traj_df_test = traj_df[traj_df.Split == 'test']\n",
    "print(traj_df_test)\n",
    "\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Weighted Distance Error\", hue=\"Model\", data=traj_df_test)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()\n",
    "\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Weighted Distance Error\", hue=\"Model\", data=traj_df_test[(traj_df.Model == 'LSTM_no_goal') | \n",
    "                                                                                            (traj_df.Model == 'LSTM_gt_goal')])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 3: Top K accuracy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_list = []\n",
    "# timesteps = np.arange(20) # TODO: hard coded for now, change later.\n",
    "\n",
    "for model in model_res_dict.keys():\n",
    "    if 'no_goal' in model:\n",
    "        continue\n",
    "    for split in model_res_dict[model].keys():\n",
    "        \n",
    "        \n",
    "        #train/test\n",
    "        \n",
    "        goal_top_1_acc = model_res_dict[model][split]['goal_top_1_acc']\n",
    "        goal_top_3_acc = model_res_dict[model][split]['goal_top_3_acc']\n",
    "        goal_top_5_acc = model_res_dict[model][split]['goal_top_5_acc']\n",
    "#         goal_entropy   = model_res_dict[model][split]['goal_entropy']\n",
    "        \n",
    "        for i_fold, (t1, t3, t5) in enumerate(zip(goal_top_1_acc, \n",
    "                                                       goal_top_3_acc,\n",
    "                                                       goal_top_5_acc)):\n",
    "            data_list.append([model, split, i_fold, 1, t1])\n",
    "            data_list.append([model, split, i_fold, 3, t3])\n",
    "            data_list.append([model, split, i_fold, 5, t5])\n",
    "            \n",
    "goal_df = pd.DataFrame(data_list, columns=['Model', 'Split', 'Fold', 'k', 'Accuracy'],dtype=float)\n",
    "goal_test_df = goal_df[goal_df.Split == 'test']\n",
    "\n",
    "# Make a bar chart out of this.\n",
    "ax = sns.barplot(x='k', y='Accuracy', hue='Model', data=goal_df.sort_values(by=['Model']))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 4: Entropy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_list = []\n",
    "\n",
    "for model in model_res_dict.keys():\n",
    "    if 'no_goal' in model:\n",
    "        continue\n",
    "    for split in model_res_dict[model].keys():\n",
    "        #train/test\n",
    "        goal_entropy   = model_res_dict[model][split]['goal_entropy']\n",
    "        \n",
    "        for i_fold, ent in enumerate(goal_entropy):\n",
    "            data_list.append([model, split, i_fold, ent])\n",
    "            \n",
    "goal_df = pd.DataFrame(data_list, columns=['Model', 'Split', 'Fold', 'Entropy'],dtype=float)\n",
    "goal_test_df = goal_df[goal_df.Split == 'test']\n",
    "\n",
    "# Make a bar chart out of this.\n",
    "ax = sns.barplot(x='Model', y='Entropy', data=goal_df.sort_values(by=['Model']))\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
