{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from scipy.special import entr # see https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.entr.html\n",
    "from keras.utils import to_categorical\n",
    "import keras.metrics as metrics\n",
    "from kf_impl import EKF_CV_MODEL\n",
    "from lstm_impl import CombinedLSTM\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to extract training data from pickles\n",
    "def extract_data(pklfile, crop=None):\n",
    "    with open(pklfile, 'rb') as f:\n",
    "        dataset_all = pickle.load(f)\n",
    "        \n",
    "    # All the history trajectoreis (x, y, heading), with shape (batch_size, sequence_length, feature_dims)\n",
    "    history_traj_data = np.array(dataset_all['features'])[:, :, :]\n",
    "\n",
    "    # All the future trajectoreis (x, y, heading), with shape (batch_size, sequence_length, feature_dims)\n",
    "    future_traj_data = np.array(dataset_all['labels'])[:, :, :-1]\n",
    "\n",
    "    # All the goal positins and occupancy (x, y, occup), with shape (batch_size, (goal_nums * feature_dims))\n",
    "    goals_position = np.array(dataset_all['goals'])\n",
    "    goals_position = goals_position.reshape((goals_position.shape[0], goals_position.shape[1] * goals_position.shape[2]))\n",
    "\n",
    "    # All intention labels, with shape (batch_size, goal_nums)\n",
    "    goal_idx = np.array(dataset_all['labels'])[:, 0, -1]\n",
    "    # Convert to one-hot and the last one is undecided (-1)\n",
    "    one_hot_goal = to_categorical(goal_idx, num_classes=33)\n",
    "    \n",
    "    if crop is not None:\n",
    "        raise NotImplementedError(\"TODO\")\n",
    "    \n",
    "    return history_traj_data, future_traj_data, goals_position, one_hot_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_splits(pklfiles_to_process, num_pkl_folds=5):\n",
    "    # Using a suboptimal approach here:\n",
    "    # Just build a list of dictionaries, where entry_i \n",
    "    # corresponds to train split i.\n",
    "\n",
    "    train_sets = []\n",
    "    test_sets  = []\n",
    "    \n",
    "    \n",
    "    inds = np.arange(num_pkl_folds)\n",
    "    \n",
    "    for hold_out_ind in inds:\n",
    "        train_inds = np.delete(inds, hold_out_ind)\n",
    "        \n",
    "        train_set = {\"history_traj_data\" : None,\n",
    "                     \"future_traj_data\"  : None,\n",
    "                     \"goal_position\"     : None,\n",
    "                     \"one_hot_goal\"      : None}\n",
    "\n",
    "        test_set  = {\"history_traj_data\" : None,\n",
    "                     \"future_traj_data\"  : None,\n",
    "                     \"goal_position\"     : None,\n",
    "                     \"one_hot_goal\"      : None}\n",
    "        \n",
    "        # Train Set\n",
    "        for fold in train_inds:\n",
    "            search_str = 'fold_%d' % fold\n",
    "            search_res = np.argwhere([search_str in f for f in pklfiles_to_process])\n",
    "            assert(len(search_res) == 1)\n",
    "            file_num = search_res[0][0]\n",
    "            history_traj_data, future_traj_data, goals_position, one_hot_goal = extract_data(pklfiles_to_process[file_num])\n",
    "\n",
    "            if train_set['history_traj_data'] is None:\n",
    "                train_set['history_traj_data'] = history_traj_data\n",
    "                train_set['future_traj_data']  = future_traj_data\n",
    "                train_set['goal_position']     = goals_position\n",
    "                train_set['one_hot_goal']      = one_hot_goal         \n",
    "            else:\n",
    "                train_set['history_traj_data'] = np.append(train_set['history_traj_data'], history_traj_data, axis=0)\n",
    "                train_set['future_traj_data']  = np.append(train_set['future_traj_data'], future_traj_data, axis=0)\n",
    "                train_set['goal_position']     = np.append(train_set['goal_position'], goals_position, axis=0)\n",
    "                train_set['one_hot_goal']      = np.append(train_set['one_hot_goal'], one_hot_goal, axis=0)\n",
    "\n",
    "        # Test Set\n",
    "        for fold in [hold_out_ind]:\n",
    "            search_str = 'fold_%d' % fold\n",
    "            search_res = np.argwhere([search_str in f for f in pklfiles_to_process])\n",
    "            assert(len(search_res) == 1)\n",
    "            file_num = search_res[0][0]\n",
    "            history_traj_data, future_traj_data, goals_position, one_hot_goal = extract_data(pklfiles_to_process[file_num])\n",
    "\n",
    "            if test_set['history_traj_data'] is None:\n",
    "                test_set['history_traj_data'] = history_traj_data\n",
    "                test_set['future_traj_data']  = future_traj_data\n",
    "                test_set['goal_position']     = goals_position\n",
    "                test_set['one_hot_goal']      = one_hot_goal         \n",
    "            else:\n",
    "                test_set['history_traj_data'] = np.append(test_set['history_traj_data'], history_traj_data, axis=0)\n",
    "                test_set['future_traj_data']  = np.append(test_set['future_traj_data'], future_traj_data, axis=0)\n",
    "                test_set['goal_position']     = np.append(test_set['goal_position'], goals_position, axis=0)\n",
    "                test_set['one_hot_goal']      = np.append(test_set['one_hot_goal'], one_hot_goal, axis=0)\n",
    "                \n",
    "        train_sets.append(train_set)\n",
    "        test_sets.append(test_set)\n",
    "        \n",
    "    return train_sets, test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_by_timestep(traj_pred, traj_actual):\n",
    "    # returns avg, min, max distance error across each timestep\n",
    "    diff = traj_pred - traj_actual # N by N_pred by 2\n",
    "    diff_xy_norm = np.linalg.norm(diff, axis=2)\n",
    "    return np.mean(diff_xy_norm, axis=0), np.min(diff_xy_norm, axis = 0), np.max(diff_xy_norm, axis=0)\n",
    "\n",
    "def top_k_accuracy(goal_pred, goal_actual, k=1):\n",
    "    # returns empirical probability of the real goal being contained\n",
    "    # in the top k most likely goal set from goal_pred.\n",
    "    return np.mean(metrics.top_k_categorical_accuracy(goal_actual, goal_pred, k=k))\n",
    "\n",
    "def mean_entropy(goal_pred):\n",
    "    # returns the avg. entropy of the goal prediction dist.\n",
    "    # higher entropy indicates more uncertain predictions\n",
    "    N = goal_pred.shape[0]\n",
    "    \n",
    "    entr_matrix = entr(goal_pred)\n",
    "    entr_by_instance = np.sum(entr_matrix, axis=1) #entropy by snippet\n",
    "    return np.mean(entr_by_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training EKF_CV, Fold 0\n",
      "1410\n",
      "353\n",
      "Training EKF_CV, Fold 1\n",
      "1410\n",
      "353\n",
      "Training EKF_CV, Fold 2\n",
      "1410\n",
      "353\n",
      "Training EKF_CV, Fold 3\n",
      "1411\n",
      "352\n",
      "Training EKF_CV, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h50_b0.001, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b0.001, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b0.001, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b0.001, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h50_b0.001, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h50_b0.100, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b0.100, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b0.100, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b0.100, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h50_b0.100, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h50_b1.000, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b1.000, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b1.000, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h50_b1.000, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h50_b1.000, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h100_b0.001, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b0.001, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b0.001, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b0.001, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h100_b0.001, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h100_b0.100, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b0.100, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b0.100, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b0.100, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h100_b0.100, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h100_b1.000, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b1.000, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b1.000, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h100_b1.000, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h100_b1.000, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h200_b0.001, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b0.001, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b0.001, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b0.001, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h200_b0.001, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h200_b0.100, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b0.100, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b0.100, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b0.100, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h200_b0.100, Fold 4\n",
      "1411\n",
      "352\n",
      "Training LSTM_h200_b1.000, Fold 0\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b1.000, Fold 1\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b1.000, Fold 2\n",
      "1410\n",
      "353\n",
      "Training LSTM_h200_b1.000, Fold 3\n",
      "1411\n",
      "352\n",
      "Training LSTM_h200_b1.000, Fold 4\n",
      "1411\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "# Construct the evaluation datasets.\n",
    "MODE = 'LOAD' # 'TRAIN' or 'LOAD'\n",
    "\n",
    "pklfiles_to_process = glob.glob('./dataset/*.pkl')\n",
    "pklfiles_to_process.sort()\n",
    "train_sets, test_sets = build_train_test_splits(pklfiles_to_process, num_pkl_folds=5)\n",
    "\n",
    "# Build the model bank.\n",
    "models = [EKF_CV_MODEL(x_init=np.zeros(5), P_init=np.eye(5), R=np.diag([1e-3]*3), dt=0.1)]\n",
    "names =  ['EKF_CV']\n",
    "\n",
    "# Create saving directories.\n",
    "if not os.path.exists('./model'):\n",
    "    os.makedirs('./model')\n",
    "if not os.path.exists('./results'):\n",
    "    os.makedirs('./results')\n",
    "\n",
    "# Build Trajectory Model\n",
    "# history_shape = train_sets[0]['history_traj_data'].shape\n",
    "# goal_position_shape = train_sets[0]['goal_position'].shape\n",
    "# one_hot_goal_shape = train_sets[0]['one_hot_goal'].shape\n",
    "# future_shape = train_sets[0]['future_traj_data'].shape\n",
    "# Hard coded for now, need to make this more robust:\n",
    "history_shape       = (None, 5, 3)\n",
    "goal_position_shape = (None, 32*3)\n",
    "one_hot_goal_shape  = (None, 32+1)\n",
    "future_shape        = (None, 20, 2)\n",
    "\n",
    "for hidden_dim in [50, 100, 200]:\n",
    "    for beta in [0.001, 0.1, 1.0]:\n",
    "        models.append(\n",
    "            CombinedLSTM(history_shape,\n",
    "                         goal_position_shape,\n",
    "                         one_hot_goal_shape,\n",
    "                         future_shape,\n",
    "                         hidden_dim,\n",
    "                         beta=beta)\n",
    "        )\n",
    "    \n",
    "        names.append('LSTM_h%d_b%.3f' % (hidden_dim, beta))\n",
    "            \n",
    "model_res_dict = {} # same indexing/length as names/models\n",
    "if mode is 'TRAIN':\n",
    "    for name, model in zip(names, models):\n",
    "        metric_dict = {}\n",
    "        metric_dict['train'] = {'N_instances'   : [],\n",
    "                                'traj_dist_vs_N': [],\n",
    "                                'goal_top_1_acc': [], \n",
    "                                'goal_top_3_acc': [],\n",
    "                                'goal_top_5_acc': [],\n",
    "                                'goal_entropy'  : []}\n",
    "        metric_dict['test']  = {'N_instances'   : [],\n",
    "                                'traj_dist_vs_N': [],\n",
    "                                'goal_top_1_acc': [], \n",
    "                                'goal_top_3_acc': [],\n",
    "                                'goal_top_5_acc': [],\n",
    "                                'goal_entropy'  : []}\n",
    "\n",
    "        for i_fold, (train_set, test_set) in enumerate(zip(train_sets, test_sets)):\n",
    "            print('Training %s, Fold %d' % (name, i_fold))\n",
    "            model.fit(train_set, test_set)\n",
    "            train_goal_pred, train_traj_pred = model.predict(train_set)\n",
    "            model.save('./model/%s_fold%d' % (name, i_fold))\n",
    "\n",
    "            for tkey, tset in zip(['train', 'test'], [train_set, test_set]):\n",
    "                goal_pred, traj_pred = model.predict(tset)\n",
    "                N_instances = goal_pred.shape[0]\n",
    "                #print(N_instances)\n",
    "\n",
    "                meand, mind, maxd = dist_by_timestep(traj_pred, tset['future_traj_data'][:,:,:2])\n",
    "\n",
    "                t1 = top_k_accuracy(goal_pred, tset['one_hot_goal'], k=1)\n",
    "                t3 = top_k_accuracy(goal_pred, tset['one_hot_goal'], k=3)\n",
    "                t5 = top_k_accuracy(goal_pred, tset['one_hot_goal'], k=5)\n",
    "\n",
    "                ment = mean_entropy(goal_pred)\n",
    "                metric_dict[tkey]['N_instances'].append(N_instances)\n",
    "                metric_dict[tkey]['traj_dist_vs_N'].append(meand)\n",
    "                metric_dict[tkey]['goal_top_1_acc'].append(t1)\n",
    "                metric_dict[tkey]['goal_top_3_acc'].append(t3)\n",
    "                metric_dict[tkey]['goal_top_5_acc'].append(t5)\n",
    "                metric_dict[tkey]['goal_entropy'].append(ment)\n",
    "\n",
    "                # TODO: save predictions/labels?\n",
    "\n",
    "        model_res_dict[name] = metric_dict\n",
    "\n",
    "    # save the model_res_dict to ./results\n",
    "    filename = 'model_comparison_dict.pkl'\n",
    "    pickle.dump(model_res_dict, open('./results/%s' % filename, 'wb'))\n",
    "else:\n",
    "    pass\n",
    "    # TODO, maybe just load results/models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODOS:\n",
    "(0) using predicted goals for LSTM model, instead of the real one.  Multimodal metric?\n",
    "(1) traj_dist_vs_N plot with std dev shading using seaborn.lineplot and pandas\n",
    "(2) top K% accuracy plot with k on x-axis, % on y-axis, and groups with error bars\n",
    "(3) entropy of the various models\n",
    "(4) code check\n",
    "\n",
    "Writing TODOs:\n",
    "(1) Methods: Tuesday (slides)\n",
    "(2) Results: Monday\n",
    "(3) Abstract/Conclusion: Wednesday\n",
    "(4) Thursday to edit and submit\n",
    "'''\n",
    "\n",
    "for name, mdict in zip(names, metric_dicts):\n",
    "    print('Model: ', name)\n",
    "    print('\\tTop 1: ')\n",
    "    print('\\t\\tMean: ', np.mean(mdict['goal_top_1_acc']))\n",
    "    print('\\t\\tStd: ' , np.std(mdict['goal_top_1_acc']))\n",
    "    print('\\t\\tMin: ' , np.min(mdict['goal_top_1_acc']))\n",
    "    print('\\t\\tMin: ' , np.max(mdict['goal_top_1_acc']))\n",
    "    \n",
    "    print('\\tTop 3: ')\n",
    "    print('\\t\\tMean: ', np.mean(mdict['goal_top_3_acc']))\n",
    "    print('\\t\\tStd: ' , np.std(mdict['goal_top_3_acc']))\n",
    "    print('\\t\\tMin: ' , np.min(mdict['goal_top_3_acc']))\n",
    "    print('\\t\\tMin: ' , np.max(mdict['goal_top_3_acc']))\n",
    "    \n",
    "    print('\\tTop 5: ')\n",
    "    print('\\t\\tMean: ', np.mean(mdict['goal_top_5_acc']))\n",
    "    print('\\t\\tStd: ' , np.std(mdict['goal_top_5_acc']))\n",
    "    print('\\t\\tMin: ' , np.min(mdict['goal_top_5_acc']))\n",
    "    print('\\t\\tMin: ' , np.max(mdict['goal_top_5_acc']))\n",
    "    \n",
    "    print('\\tEntropy: ')\n",
    "    print('\\t\\tMean: ', np.mean(mdict['goal_entropy']))\n",
    "    print('\\t\\tStd: ' , np.std(mdict['goal_entropy']))\n",
    "    print('\\t\\tMin: ' , np.min(mdict['goal_entropy']))\n",
    "    print('\\t\\tMin: ' , np.max(mdict['goal_entropy']))\n",
    "    \n",
    "    N_pred = 20 # hard coded for now\n",
    "    \n",
    "    for i_fold in range(len(test_sets)):\n",
    "        if 'EKF' in name:\n",
    "            plt.plot(np.arange(N_pred), mdict['traj_dist_vs_N'][i_fold], 'k')\n",
    "        else:\n",
    "            plt.plot(np.arange(N_pred), mdict['traj_dist_vs_N'][i_fold], 'b')\n",
    "\n",
    "plt.title('Avg. Distance Error vs. Timestep')\n",
    "plt.xlabel('Timestep k')\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
