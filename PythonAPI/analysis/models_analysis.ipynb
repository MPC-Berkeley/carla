{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from scipy.special import entr # see https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.entr.html\n",
    "from keras.utils import to_categorical\n",
    "import keras.metrics as metrics\n",
    "from kf_impl import EKF_CV_MODEL\n",
    "from lstm_impl import CombinedLSTM\n",
    "import pdb\n",
    "from utils import extract_data, sup_plot\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_splits(pklfiles_to_process, num_pkl_folds=5):\n",
    "    # Using a suboptimal approach here:\n",
    "    # Just build a list of dictionaries, where entry_i \n",
    "    # corresponds to train split i.\n",
    "\n",
    "    train_sets = []\n",
    "    test_sets  = []\n",
    "    \n",
    "    \n",
    "    inds = np.arange(num_pkl_folds)\n",
    "    \n",
    "    for hold_out_ind in inds:\n",
    "        train_inds = np.delete(inds, hold_out_ind)\n",
    "        \n",
    "        train_set = {\"history_traj_data\" : None,\n",
    "                     \"future_traj_data\"  : None,\n",
    "                     \"goal_position\"     : None,\n",
    "                     \"one_hot_goal\"      : None}\n",
    "\n",
    "        test_set  = {\"history_traj_data\" : None,\n",
    "                     \"future_traj_data\"  : None,\n",
    "                     \"goal_position\"     : None,\n",
    "                     \"one_hot_goal\"      : None}\n",
    "        \n",
    "        # Train Set\n",
    "        for fold in train_inds:\n",
    "            search_str = 'fold_%d' % fold\n",
    "            search_res = np.argwhere([search_str in f for f in pklfiles_to_process])\n",
    "            assert(len(search_res) == 1)\n",
    "            file_num = search_res[0][0]\n",
    "            \n",
    "            print(\"Processing %s\" % pklfiles_to_process[file_num])\n",
    "            history_traj_data, future_traj_data, goals_position, one_hot_goal = extract_data(pklfiles_to_process[file_num])\n",
    "\n",
    "            if train_set['history_traj_data'] is None:\n",
    "                train_set['history_traj_data'] = history_traj_data\n",
    "                train_set['future_traj_data']  = future_traj_data\n",
    "                train_set['goal_position']     = goals_position\n",
    "                train_set['one_hot_goal']      = one_hot_goal         \n",
    "            else:\n",
    "                train_set['history_traj_data'] = np.append(train_set['history_traj_data'], history_traj_data, axis=0)\n",
    "                train_set['future_traj_data']  = np.append(train_set['future_traj_data'], future_traj_data, axis=0)\n",
    "                train_set['goal_position']     = np.append(train_set['goal_position'], goals_position, axis=0)\n",
    "                train_set['one_hot_goal']      = np.append(train_set['one_hot_goal'], one_hot_goal, axis=0)\n",
    "\n",
    "        # Test Set\n",
    "        for fold in [hold_out_ind]:\n",
    "            search_str = 'fold_%d' % fold\n",
    "            search_res = np.argwhere([search_str in f for f in pklfiles_to_process])\n",
    "            assert(len(search_res) == 1)\n",
    "            file_num = search_res[0][0]\n",
    "            history_traj_data, future_traj_data, goals_position, one_hot_goal = extract_data(pklfiles_to_process[file_num])\n",
    "\n",
    "            if test_set['history_traj_data'] is None:\n",
    "                test_set['history_traj_data'] = history_traj_data\n",
    "                test_set['future_traj_data']  = future_traj_data\n",
    "                test_set['goal_position']     = goals_position\n",
    "                test_set['one_hot_goal']      = one_hot_goal         \n",
    "            else:\n",
    "                test_set['history_traj_data'] = np.append(test_set['history_traj_data'], history_traj_data, axis=0)\n",
    "                test_set['future_traj_data']  = np.append(test_set['future_traj_data'], future_traj_data, axis=0)\n",
    "                test_set['goal_position']     = np.append(test_set['goal_position'], goals_position, axis=0)\n",
    "                test_set['one_hot_goal']      = np.append(test_set['one_hot_goal'], one_hot_goal, axis=0)\n",
    "                \n",
    "        train_sets.append(train_set)\n",
    "        test_sets.append(test_set)\n",
    "        \n",
    "    return train_sets, test_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_dist_by_timestep(goal_pred, traj_pred_dict, traj_actual):\n",
    "    # M = # of instances, N = time horizon, 2 (xy) \n",
    "    M = traj_pred_dict[0].shape[0]\n",
    "    N = traj_pred_dict[0].shape[1]\n",
    "    \n",
    "    weighted_sum = np.zeros((M, N))\n",
    "    num_pred_traj = len(traj_pred_dict.keys())\n",
    "    top_k_probs = -np.sort(-goal_pred, axis=1)[:,:num_pred_traj]\n",
    "    \n",
    "    for k in range(num_pred_traj):\n",
    "        # key = 0 \n",
    "        traj_pred_k = traj_pred_dict[k] # M by N by 2\n",
    "        diff = traj_pred_k - traj_actual # M by N by 2\n",
    "        diff_xy_norm = np.linalg.norm(diff, axis=2) # M by N\n",
    "\n",
    "        for i in range(N):\n",
    "            diff_xy_norm[:,i] *= top_k_probs[:,k]\n",
    "        \n",
    "        weighted_sum += diff_xy_norm\n",
    "    return np.mean(weighted_sum, axis=0)\n",
    "\n",
    "def dist_by_timestep(traj_pred_dict, traj_actual):\n",
    "    # returns avg, min, max distance error across each timestep\n",
    "    diff = traj_pred_dict[0] - traj_actual # N by N_pred by 2\n",
    "    diff_xy_norm = np.linalg.norm(diff, axis=2)\n",
    "    return np.mean(diff_xy_norm, axis=0), np.min(diff_xy_norm, axis = 0), np.max(diff_xy_norm, axis=0)\n",
    "\n",
    "def top_k_accuracy(goal_pred, goal_actual, k=1):\n",
    "    # returns empirical probability of the real goal being contained\n",
    "    # in the top k most likely goal set from goal_pred.\n",
    "    return np.mean(metrics.top_k_categorical_accuracy(goal_actual, goal_pred, k=k))\n",
    "\n",
    "def mean_entropy(goal_pred):\n",
    "    # returns the avg. entropy of the goal prediction dist.\n",
    "    # higher entropy indicates more uncertain predictions\n",
    "    N = goal_pred.shape[0]\n",
    "    \n",
    "    entr_matrix = entr(goal_pred)\n",
    "    entr_by_instance = np.sum(entr_matrix, axis=1) #entropy by snippet\n",
    "    return np.mean(entr_by_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./dataset/dataset_01_18_18:09:59_fold_1.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_2.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_3.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_4.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_0.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_2.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_3.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_4.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_0.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_1.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_3.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_4.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_0.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_1.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_2.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_4.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_0.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_1.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_2.pkl\n",
      "Processing ./dataset/dataset_01_18_18:09:59_fold_3.pkl\n",
      "Training EKF_CV, Fold 0, Time 2020-01-18-18-16-27\n",
      "Training EKF_CV, Fold 1, Time 2020-01-18-18-16-49\n",
      "Training EKF_CV, Fold 2, Time 2020-01-18-18-17-10\n",
      "Training EKF_CV, Fold 3, Time 2020-01-18-18-17-31\n",
      "Training EKF_CV, Fold 4, Time 2020-01-18-18-17-53\n",
      "Training LSTM_b0.100_g1.000, Fold 0, Time 2020-01-18-18-18-14\n",
      "Training LSTM_b0.100_g1.000, Fold 1, Time 2020-01-18-18-33-23\n",
      "Training LSTM_b0.100_g1.000, Fold 2, Time 2020-01-18-18-48-37\n",
      "Training LSTM_b0.100_g1.000, Fold 3, Time 2020-01-18-19-04-03\n",
      "Training LSTM_b0.100_g1.000, Fold 4, Time 2020-01-18-19-19-07\n"
     ]
    }
   ],
   "source": [
    "# Construct the evaluation datasets.\n",
    "MODE = 'TRAIN' # 'TRAIN' or 'LOAD'\n",
    "res_filename = 'model_comparison_dict.pkl'\n",
    "\n",
    "pklfiles_to_process = glob.glob('./dataset/dataset_01_18*.pkl')\n",
    "pklfiles_to_process.sort()\n",
    "train_sets, test_sets = build_train_test_splits(pklfiles_to_process, num_pkl_folds=5)\n",
    "\n",
    "# Build the model bank.\n",
    "models = [EKF_CV_MODEL(x_init=np.zeros(5), P_init=np.eye(5), R=np.diag([1e-3]*3), dt=0.1)]\n",
    "names =  ['EKF_CV']\n",
    "\n",
    "# Create saving directories.\n",
    "if not os.path.exists('./model'):\n",
    "    os.makedirs('./model')\n",
    "if not os.path.exists('./results'):\n",
    "    os.makedirs('./results')\n",
    "\n",
    "# Build Trajectory Model\n",
    "# history_shape = train_sets[0]['history_traj_data'].shape\n",
    "# goal_position_shape = train_sets[0]['goal_position'].shape\n",
    "# one_hot_goal_shape = train_sets[0]['one_hot_goal'].shape\n",
    "# future_shape = train_sets[0]['future_traj_data'].shape\n",
    "# Hard coded for now, need to make this more robust:\n",
    "history_shape       = (None, 5, 3)\n",
    "goal_position_shape = (None, 32*3)\n",
    "one_hot_goal_shape  = (None, 32+1)\n",
    "future_shape        = (None, 20, 2)\n",
    "hidden_dim = 100\n",
    "top_k_goal = [0,1,2]\n",
    "\n",
    "for gamma in [1.0]:\n",
    "    for beta in [0.1, 0.5, 1.0]:\n",
    "        models.append(\n",
    "            CombinedLSTM(history_shape,\n",
    "                         goal_position_shape,\n",
    "                         one_hot_goal_shape,\n",
    "                         future_shape,\n",
    "                         hidden_dim,\n",
    "                         beta=beta,\n",
    "                         gamma=gamma,\n",
    "                         use_goal_info=True)\n",
    "        )\n",
    "    \n",
    "        names.append('LSTM_b%.3f_g%.3f' % (beta, gamma)) # ground truth goal, anyone can be used for traj LSTM\n",
    "\n",
    "models.append(\n",
    "    CombinedLSTM(history_shape,\n",
    "                 goal_position_shape,\n",
    "                 one_hot_goal_shape,\n",
    "                 future_shape,\n",
    "                 hidden_dim,\n",
    "                 beta=beta,\n",
    "                 gamma=gamma,\n",
    "                 use_goal_info=False)\n",
    ") # no goal provided, beta gamma irrelevant, don't use for goal classification\n",
    "\n",
    "names.append('LSTM_no_goal')\n",
    "            \n",
    "model_res_dict = {} # same indexing/length as names/models\n",
    "if MODE is 'TRAIN':\n",
    "    for name, model in zip(names, models):\n",
    "        metric_dict = {}\n",
    "        metric_dict['train'] = {'N_instances'   : [],\n",
    "                                'traj_dist_vs_N': [],   # no goal\n",
    "                                'wtraj_dist_vs_N': [],  # weighted, multimodal\n",
    "                                'gtraj_dist_vs_N': [],  # gt\n",
    "                                'goal_top_1_acc': [], \n",
    "                                'goal_top_3_acc': [],\n",
    "                                'goal_top_5_acc': [],\n",
    "                                'goal_entropy'  : []}\n",
    "        metric_dict['test']  = {'N_instances'   : [],\n",
    "                                'traj_dist_vs_N': [],   # no goal\n",
    "                                'wtraj_dist_vs_N': [],  # weighted, multimodal\n",
    "                                'gtraj_dist_vs_N': [],  # gt\n",
    "                                'goal_top_1_acc': [], \n",
    "                                'goal_top_3_acc': [],\n",
    "                                'goal_top_5_acc': [],\n",
    "                                'goal_entropy'  : []}\n",
    "\n",
    "        for i_fold, (train_set, test_set) in enumerate(zip(train_sets, test_sets)):\n",
    "            now = datetime.datetime.now()\n",
    "            now = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "            print('Training %s, Fold %d, Time %s' % (name, i_fold, now))\n",
    "            model.fit(train_set, test_set)\n",
    "            model.save('./model/%s_fold%d' % (name, i_fold))\n",
    "\n",
    "            for tkey, tset in zip(['train', 'test'], [train_set, test_set]):\n",
    "\n",
    "                goal_pred, traj_pred_dict = model.predict(tset) # either no goal or ground truth\n",
    "                N_instances = goal_pred.shape[0]\n",
    "                \n",
    "                if 'no_goal' in name or 'EKF_CV' in name:\n",
    "                    # just populate the traj_dist_vs_N\n",
    "                    meand, mind, maxd = dist_by_timestep(traj_pred_dict, tset['future_traj_data'][:,:,:2])\n",
    "                    metric_dict[tkey]['traj_dist_vs_N'].append(meand)\n",
    "                else:\n",
    "                    meand, mind, maxd = dist_by_timestep(traj_pred_dict, tset['future_traj_data'][:,:,:2])\n",
    "                    metric_dict[tkey]['gtraj_dist_vs_N'].append(meand)\n",
    "                    \n",
    "                    goal_pred, traj_pred_dict = model.predict(tset, top_k_goal)\n",
    "                    wmeand = weighted_dist_by_timestep(goal_pred, traj_pred_dict, tset['future_traj_data'][:,:,:2])\n",
    "                    metric_dict[tkey]['wtraj_dist_vs_N'].append(wmeand)\n",
    "                    # populate gtraj_dist_vs_N (gt)\n",
    "                    # and do multimodal\n",
    "                \n",
    "                t1 = top_k_accuracy(goal_pred, tset['one_hot_goal'], k=1)\n",
    "                t3 = top_k_accuracy(goal_pred, tset['one_hot_goal'], k=3)\n",
    "                t5 = top_k_accuracy(goal_pred, tset['one_hot_goal'], k=5)\n",
    "\n",
    "                ment = mean_entropy(goal_pred)\n",
    "                metric_dict[tkey]['N_instances'].append(N_instances)\n",
    "                metric_dict[tkey]['goal_top_1_acc'].append(t1)\n",
    "                metric_dict[tkey]['goal_top_3_acc'].append(t3)\n",
    "                metric_dict[tkey]['goal_top_5_acc'].append(t5)\n",
    "                metric_dict[tkey]['goal_entropy'].append(ment)\n",
    "\n",
    "                # TODO: save predictions/labels?\n",
    "\n",
    "        model_res_dict[name] = metric_dict\n",
    "\n",
    "    # save the model_res_dict to ./results\n",
    "    filename = 'model_comparison_dict_0118.pkl'\n",
    "    pickle.dump(model_res_dict, open('./results/%s' % res_filename, 'wb'))\n",
    "elif MODE is 'LOAD':\n",
    "    # TODO: maybe load models if needed?\n",
    "    model_res_dict = pickle.load(open('./results/%s' % res_filename, 'rb'))\n",
    "else:\n",
    "    raise ValueError(\"Invalid mode!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# Get the data\n",
    "pklfiles_to_process = glob.glob('./dataset/*.pkl')\n",
    "pklfiles_to_process.sort()\n",
    "print('Found %d pkl files: %s' % (len(pklfiles_to_process), pklfiles_to_process))\n",
    "\n",
    "file_num = 0\n",
    "\n",
    "pklfile = pklfiles_to_process[file_num]\n",
    "\n",
    "vtest_set  = {\"history_traj_data\" : None,\n",
    "             \"future_traj_data\"  : None,\n",
    "             \"goal_position\"     : None,\n",
    "             \"one_hot_goal\"      : None}\n",
    "vtest_set_kf  = {\"history_traj_data\" : None,\n",
    "             \"future_traj_data\"  : None,\n",
    "             \"goal_position\"     : None,\n",
    "             \"one_hot_goal\"      : None}\n",
    "\n",
    "vtest_set['history_traj_data'], vtest_set['future_traj_data'], vtest_set['goal_position'], vtest_set['one_hot_goal'], traj_idx = extract_data(pklfile, full_traj=True, crop_traj=True)\n",
    "vtest_set_kf['history_traj_data'], vtest_set_kf['future_traj_data'], vtest_set_kf['goal_position'], vtest_set_kf['one_hot_goal'], traj_idx_kf = extract_data(pklfile, full_traj=True, crop_traj=False)\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    if 'EKF_CV' in name:\n",
    "        goal_pred, traj_pred_dict = model.predict(vtest_set_kf)\n",
    "    elif 'no_goal' in name:\n",
    "        continue\n",
    "    else:\n",
    "        goal_pred, traj_pred_dict = model.predict(vtest_set, top_k_goal=top_k_goal)\n",
    "        \n",
    "    sup_plot(name, vtest_set, traj_idx, goal_pred, traj_pred_dict, limit=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 1: timestep vs. mean distance error\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_list = []\n",
    "timesteps = np.arange(20) # TODO: hard coded for now, change later.\n",
    "\n",
    "for model in model_res_dict.keys():\n",
    "    for split in model_res_dict[model].keys(): \n",
    "        # train/test\n",
    "        if 'no_goal' in model:\n",
    "            name = model\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['traj_dist_vs_N']\n",
    "        elif 'EKF' in model:\n",
    "            name = model\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['traj_dist_vs_N']\n",
    "            # hack to only get ground truth goal based traj. pred once\n",
    "        elif 'b0.100_g0.100' in model:\n",
    "            name = 'LSTM_gt_goal'\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['gtraj_dist_vs_N']\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        for i_fold, td in enumerate(traj_dist_vs_N):\n",
    "            for j_timestep, dist_timestep in enumerate(td):\n",
    "                data_list.append([name, split, i_fold, j_timestep, dist_timestep])\n",
    "            \n",
    "traj_df = pd.DataFrame(data_list, columns=['Model', 'Split', 'Fold', 'Timestep', 'Distance Error'],dtype=float)\n",
    "print(traj_df)\n",
    "\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Distance Error\", hue=\"Model\", data=traj_df[traj_df.Split == 'test'])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()\n",
    "\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Distance Error\", hue=\"Model\", data=traj_df[(traj_df.Split == 'test') & \\\n",
    "                                                                              (traj_df.Model != 'EKF_CV')])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 2: timestep vs. weighted mean distance error\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_list = []\n",
    "timesteps = np.arange(20) # TODO: hard coded for now, change later.\n",
    "\n",
    "for model in model_res_dict.keys():\n",
    "    for split in model_res_dict[model].keys(): \n",
    "        # train/test\n",
    "        if 'no_goal' in model:\n",
    "            name = model\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['traj_dist_vs_N']\n",
    "        elif 'EKF' in model:\n",
    "            continue\n",
    "        else:\n",
    "            name = model\n",
    "            traj_dist_vs_N = model_res_dict[model][split]['wtraj_dist_vs_N']  \n",
    "            \n",
    "        for i_fold, td in enumerate(traj_dist_vs_N):\n",
    "            for j_timestep, dist_timestep in enumerate(td):\n",
    "                data_list.append([name, split, i_fold, j_timestep, dist_timestep])\n",
    "\n",
    "gtraj_dist_vs_N_test = model_res_dict['LSTM_b0.100_g0.100']['test'] ['gtraj_dist_vs_N'] \n",
    "for i_fold, td in enumerate(gtraj_dist_vs_N_test):\n",
    "    for j_timestep, dist_timestep in enumerate(td):\n",
    "        data_list.append(['LSTM_gt_goal', 'test', i_fold, j_timestep, dist_timestep])\n",
    "\n",
    "gtraj_dist_vs_N_train = model_res_dict['LSTM_b0.100_g0.001']['train']['gtraj_dist_vs_N'] \n",
    "for i_fold, td in enumerate(gtraj_dist_vs_N_train):\n",
    "    for j_timestep, dist_timestep in enumerate(td):\n",
    "        data_list.append(['LSTM_gt_goal', 'train', i_fold, j_timestep, dist_timestep])\n",
    "                \n",
    "traj_df = pd.DataFrame(data_list, columns=['Model', 'Split', 'Fold', 'Timestep', 'Weighted Distance Error'],dtype=float)\n",
    "traj_df_test = traj_df[traj_df.Split == 'test']\n",
    "print(traj_df_test)\n",
    "\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Weighted Distance Error\", hue=\"Model\", data=traj_df_test)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()\n",
    "\n",
    "ax = sns.lineplot(x=\"Timestep\", y=\"Weighted Distance Error\", hue=\"Model\", data=traj_df_test[(traj_df.Model == 'LSTM_no_goal') | \n",
    "                                                                                            (traj_df.Model == 'LSTM_gt_goal')])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Distance Error (m)')\n",
    "plt.xticks(np.arange(0, 21, step=2))\n",
    "plt.yticks(np.arange(0, 1.51, step=0.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 3: Top K accuracy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_list = []\n",
    "# timesteps = np.arange(20) # TODO: hard coded for now, change later.\n",
    "\n",
    "for model in model_res_dict.keys():\n",
    "    if 'no_goal' in model:\n",
    "        continue\n",
    "    for split in model_res_dict[model].keys():\n",
    "        \n",
    "        \n",
    "        #train/test\n",
    "        \n",
    "        goal_top_1_acc = model_res_dict[model][split]['goal_top_1_acc']\n",
    "        goal_top_3_acc = model_res_dict[model][split]['goal_top_3_acc']\n",
    "        goal_top_5_acc = model_res_dict[model][split]['goal_top_5_acc']\n",
    "#         goal_entropy   = model_res_dict[model][split]['goal_entropy']\n",
    "        \n",
    "        for i_fold, (t1, t3, t5) in enumerate(zip(goal_top_1_acc, \n",
    "                                                       goal_top_3_acc,\n",
    "                                                       goal_top_5_acc)):\n",
    "            data_list.append([model, split, i_fold, 1, t1])\n",
    "            data_list.append([model, split, i_fold, 3, t3])\n",
    "            data_list.append([model, split, i_fold, 5, t5])\n",
    "            \n",
    "goal_df = pd.DataFrame(data_list, columns=['Model', 'Split', 'Fold', 'k', 'Accuracy'],dtype=float)\n",
    "goal_test_df = goal_df[goal_df.Split == 'test']\n",
    "\n",
    "# Make a bar chart out of this.\n",
    "ax = sns.barplot(x='k', y='Accuracy', hue='Model', data=goal_df.sort_values(by=['Model']))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT 4: Entropy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "data_list = []\n",
    "\n",
    "for model in model_res_dict.keys():\n",
    "    if 'no_goal' in model:\n",
    "        continue\n",
    "    for split in model_res_dict[model].keys():\n",
    "        #train/test\n",
    "        goal_entropy   = model_res_dict[model][split]['goal_entropy']\n",
    "        \n",
    "        for i_fold, ent in enumerate(goal_entropy):\n",
    "            data_list.append([model, split, i_fold, ent])\n",
    "            \n",
    "goal_df = pd.DataFrame(data_list, columns=['Model', 'Split', 'Fold', 'Entropy'],dtype=float)\n",
    "goal_test_df = goal_df[goal_df.Split == 'test']\n",
    "\n",
    "# Make a bar chart out of this.\n",
    "ax = sns.barplot(x='Model', y='Entropy', data=goal_df.sort_values(by=['Model']))\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
