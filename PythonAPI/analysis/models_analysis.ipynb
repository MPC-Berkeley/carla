{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from kf_impl import EKF_CV_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to extract training data from pickles\n",
    "def extract_data(pklfile, crop=None):\n",
    "    with open(pklfile, 'rb') as f:\n",
    "        dataset_all = pickle.load(f)\n",
    "        \n",
    "    # All the history trajectoreis (x, y, heading), with shape (batch_size, sequence_length, feature_dims)\n",
    "    history_traj_data = np.array(dataset_all['features'])[:, :, :]\n",
    "\n",
    "    # All the future trajectoreis (x, y, heading), with shape (batch_size, sequence_length, feature_dims)\n",
    "    future_traj_data = np.array(dataset_all['labels'])[:, :, :-1]\n",
    "\n",
    "    # All the goal positins and occupancy (x, y, occup), with shape (batch_size, (goal_nums * feature_dims))\n",
    "    goals_position = np.array(dataset_all['goals'])\n",
    "    goals_position = goals_position.reshape((goals_position.shape[0], goals_position.shape[1] * goals_position.shape[2]))\n",
    "\n",
    "    # All intention labels, with shape (batch_size, goal_nums)\n",
    "    goal_idx = np.array(dataset_all['labels'])[:, 0, -1]\n",
    "    # Convert to one-hot and the last one is undecided (-1)\n",
    "    one_hot_goal = to_categorical(goal_idx, num_classes=33)\n",
    "    \n",
    "    if crop is not None:\n",
    "        raise NotImplementedError(\"TODO\")\n",
    "    \n",
    "    return history_traj_data, future_traj_data, goals_position, one_hot_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_splits(pklfiles_to_process, num_pkl_folds=5):\n",
    "    # Using a suboptimal approach here:\n",
    "    # Just build a list of dictionaries, where entry_i \n",
    "    # corresponds to train split i.\n",
    "\n",
    "    train_sets = []\n",
    "    test_sets  = []\n",
    "    \n",
    "    \n",
    "    inds = np.arange(num_pkl_folds)\n",
    "    \n",
    "    for hold_out_ind in inds:\n",
    "        train_inds = np.delete(inds, hold_out_ind)\n",
    "        \n",
    "        train_set = {\"history_traj_data\" : None,\n",
    "                     \"future_traj_data\"  : None,\n",
    "                     \"goal_position\"     : None,\n",
    "                     \"one_hot_goal\"      : None}\n",
    "\n",
    "        test_set  = {\"history_traj_data\" : None,\n",
    "                     \"future_traj_data\"  : None,\n",
    "                     \"goal_position\"     : None,\n",
    "                     \"one_hot_goal\"      : None}\n",
    "        \n",
    "        # Train Set\n",
    "        for fold in train_inds:\n",
    "            search_str = 'fold_%d' % fold\n",
    "            search_res = np.argwhere([search_str in f for f in pklfiles_to_process])\n",
    "            assert(len(search_res) == 1)\n",
    "            file_num = search_res[0][0]\n",
    "            history_traj_data, future_traj_data, goals_position, one_hot_goal = extract_data(pklfiles_to_process[file_num])\n",
    "\n",
    "            if train_set['history_traj_data'] is None:\n",
    "                train_set['history_traj_data'] = history_traj_data\n",
    "                train_set['future_traj_data']  = future_traj_data\n",
    "                train_set['goal_position']     = goals_position\n",
    "                train_set['one_hot_goal']      = one_hot_goal         \n",
    "            else:\n",
    "                train_set['history_traj_data'] = np.append(train_set['history_traj_data'], history_traj_data, axis=0)\n",
    "                train_set['future_traj_data']  = np.append(train_set['future_traj_data'], future_traj_data, axis=0)\n",
    "                train_set['goal_position']     = np.append(train_set['goal_position'], goals_position, axis=0)\n",
    "                train_set['one_hot_goal']      = np.append(train_set['one_hot_goal'], one_hot_goal, axis=0)\n",
    "\n",
    "        # Test Set\n",
    "        for fold in [hold_out_ind]:\n",
    "            search_str = 'fold_%d' % fold\n",
    "            search_res = np.argwhere([search_str in f for f in pklfiles_to_process])\n",
    "            assert(len(search_res) == 1)\n",
    "            file_num = search_res[0][0]\n",
    "            history_traj_data, future_traj_data, goals_position, one_hot_goal = extract_data(pklfiles_to_process[file_num])\n",
    "\n",
    "            if test_set['history_traj_data'] is None:\n",
    "                test_set['history_traj_data'] = history_traj_data\n",
    "                test_set['future_traj_data']  = future_traj_data\n",
    "                test_set['goal_position']     = goals_position\n",
    "                test_set['one_hot_goal']      = one_hot_goal         \n",
    "            else:\n",
    "                test_set['history_traj_data'] = np.append(test_set['history_traj_data'], history_traj_data, axis=0)\n",
    "                test_set['future_traj_data']  = np.append(test_set['future_traj_data'], future_traj_data, axis=0)\n",
    "                test_set['goal_position']     = np.append(test_set['goal_position'], goals_position, axis=0)\n",
    "                test_set['one_hot_goal']      = np.append(test_set['one_hot_goal'], one_hot_goal, axis=0)\n",
    "                \n",
    "        train_sets.append(train_set)\n",
    "        test_sets.append(test_set)\n",
    "        \n",
    "    return train_sets, test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q identified as:  [[ 5.79e-04  3.29e-05 -5.16e-05 -7.03e-05  6.83e-05]\n",
      " [ 3.29e-05  1.35e-03 -2.09e-08 -7.97e-04 -2.22e-05]\n",
      " [-5.16e-05 -2.09e-08  2.83e-03  8.11e-06  2.00e-05]\n",
      " [-7.03e-05 -7.97e-04  8.11e-06  1.71e-02  4.29e-05]\n",
      " [ 6.83e-05 -2.22e-05  2.00e-05  4.29e-05  4.46e-04]]\n",
      "Q identified as:  [[ 5.88e-04  3.03e-05 -2.71e-05 -4.42e-05  7.15e-05]\n",
      " [ 3.03e-05  1.37e-03  2.91e-06 -7.71e-04 -1.81e-05]\n",
      " [-2.71e-05  2.91e-06  2.24e-03  1.18e-05  2.25e-05]\n",
      " [-4.42e-05 -7.71e-04  1.18e-05  1.64e-02  6.86e-05]\n",
      " [ 7.15e-05 -1.81e-05  2.25e-05  6.86e-05  4.50e-04]]\n",
      "Q identified as:  [[ 6.08e-04  2.52e-05 -2.08e-05 -2.39e-06  7.43e-05]\n",
      " [ 2.52e-05  1.35e-03  2.98e-06 -7.80e-04 -2.15e-05]\n",
      " [-2.08e-05  2.98e-06  2.31e-03  1.16e-05  2.18e-05]\n",
      " [-2.39e-06 -7.80e-04  1.16e-05  1.73e-02  7.45e-05]\n",
      " [ 7.43e-05 -2.15e-05  2.18e-05  7.45e-05  4.75e-04]]\n",
      "Q identified as:  [[ 5.92e-04  2.74e-05 -5.39e-05 -5.47e-05  7.26e-05]\n",
      " [ 2.74e-05  1.35e-03 -1.82e-06 -8.01e-04 -2.34e-05]\n",
      " [-5.39e-05 -1.82e-06  1.66e-03  1.68e-05  2.28e-05]\n",
      " [-5.47e-05 -8.01e-04  1.68e-05  1.69e-02  9.47e-05]\n",
      " [ 7.26e-05 -2.34e-05  2.28e-05  9.47e-05  4.60e-04]]\n",
      "Q identified as:  [[ 6.01e-04  3.12e-05 -2.97e-05 -3.87e-05  6.82e-05]\n",
      " [ 3.12e-05  1.37e-03 -7.77e-06 -7.83e-04 -2.55e-05]\n",
      " [-2.97e-05 -7.77e-06  2.28e-03 -3.48e-06  2.16e-05]\n",
      " [-3.87e-05 -7.83e-04 -3.48e-06  1.66e-02  1.30e-04]\n",
      " [ 6.82e-05 -2.55e-05  2.16e-05  1.30e-04  4.39e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Construct the evaluation datasets.\n",
    "pklfiles_to_process = glob.glob('./dataset/*.pkl')\n",
    "pklfiles_to_process.sort()\n",
    "train_sets, test_sets = build_train_test_splits(pklfiles_to_process, num_pkl_folds=5)\n",
    "\n",
    "# Build the model bank.\n",
    "models = [] #TODO, LSTM + EKF\n",
    "model = EKF_CV_MODEL(x_init=np.zeros(5),\n",
    "                     P_init=np.eye(5),\n",
    "                     R=np.diag([1e-3]*3),\n",
    "                     dt=0.1)\n",
    "\n",
    "for train_set, test_set in zip(train_sets, test_sets):\n",
    "    model.fit(train_set, test_set)\n",
    "    # model save?\n",
    "    goal_pred, traj_pred = model.predict(test_set)\n",
    "\n",
    "    # TODO: compute metrics and store in an array for final results.\n",
    "    # keras/sklearn metrics used for common evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: port Xu's visualization code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
