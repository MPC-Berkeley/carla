{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes processed mat/pickle files and makes a dataset for prediction.\n",
    "In particular, given a history and prediction horizon, this generates all viable slices of a full trajectory.  This goes through all pickle files and constructs a dataset for k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from pkl_reader import *\n",
    "from datetime import datetime\n",
    "import sklearn.utils as sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CONFIG: CONSTANTS FOR EXECUTION '''\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime('%m_%d_%H:%M:%S')\n",
    "dataset_name = 'pilot_dataset_%s' % dt_string\n",
    "shuffle = True   # if True, shuffle snippets according to seed (for test/train split)\n",
    "seed = 0;\n",
    "num_folds_cv = 5 # k-fold cross validation, number of splits (-1 = only provide full set of data)\n",
    "\n",
    "prune_start=True          # remove stationary portion of ego's trajectory at the start\n",
    "prune_end=True            # remove stationary portion of ego's trajectory at the end\n",
    "min_vel_thresh=0.01       # velocity threshold (m/s) above which ego is considered moving\n",
    "exclude_collisions=True  # return an empty trajectory if there was a collision\n",
    "\n",
    "Nhist=5          # number of timesteps of motion history to predict with\n",
    "Npred=20         # number of timesteps of prediction horizon\n",
    "Nskip=5          # \"stride\" for sliding window of snippet selection\n",
    "dt=0.1           # discretization (s) of full ego trajectory corresponding to N* above\n",
    "ego_trans = True # whether or not to represent trajectory snippets in the ego frame\n",
    "                 # if False, use the global map frame for all snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* [ ] optional: complete outlier removal in bag_processing (see \"Additional Notes\").  Need to check time indices.\n",
    "* [ ] optional: include source pkl name for snippet to trial association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files to read: ['../examples/bags/parking_p1_t1_e2_2019-11-04-14-35-28.pkl', '../examples/bags/parking_p2_t2_e0_2019-11-04-16-36-50.pkl']\n"
     ]
    }
   ],
   "source": [
    "save_ext = 'pkl'\n",
    "files_to_process = glob.glob('../examples/bags/*.%s' % save_ext)\n",
    "\n",
    "print('Found %d files to read: %s' % (len(files_to_process), files_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing\n",
      "\n",
      "Collisions encountered: \n",
      "time 1218.723575582\n",
      "other_id 868 vehicle.ford.mustang_autopilot\n",
      "normal_impulse [-301.71484375, -1832.911865234375, 34.343990325927734]\n",
      "time 1218.823575587\n",
      "other_id 868 vehicle.ford.mustang_autopilot\n",
      "normal_impulse [-42.54573059082031, -468.8822937011719, 8.161503791809082]\n",
      "../examples/bags/parking_p2_t2_e0_2019-11-04-16-36-50.pkl Collision encountered, so skipping this instance.\n",
      "Finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Processing\\n')\n",
    "features_combined = []\n",
    "labels_combined = []\n",
    "goal_snippets_combined = []\n",
    "\n",
    "for file in files_to_process:\n",
    "    if save_ext == 'pkl':\n",
    "        res_dict = pickle.load(open(file,'rb'))\n",
    "    else:\n",
    "        raise NotImplemented('Invalid Extension')\n",
    "    \n",
    "    goals = extract_goals(res_dict)\n",
    "    \n",
    "    try:\n",
    "        ego_trajectory, start_ind, switch_ind, end_ind, goal_ind = \\\n",
    "             extract_full_trajectory(res_dict, goals, prune_start, prune_end, \\\n",
    "                                     min_vel_thresh, exclude_collisions)\n",
    "\n",
    "        features, labels, goal_snpts = \\\n",
    "            get_ego_trajectory_prediction_snippets(ego_trajectory, start_ind, switch_ind, end_ind, goal_ind, \\\n",
    "                                           goals, Nhist, Npred, Nskip, dt, ego_frame=ego_trans)\n",
    "        \n",
    "        features_combined.extend(features)\n",
    "        labels_combined.extend(labels)\n",
    "        goal_snippets_combined.extend(goal_snpts)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(file, e)\n",
    "assert(len(features_combined) == len(labels_combined) == len(goal_snippets_combined))\n",
    "\n",
    "if shuffle:\n",
    "    features_combined, labels_combined, goal_snippets_combined = \\\n",
    "    sku.shuffle(features_combined, labels_combined, goal_snippets_combined, random_state=seed)\n",
    "\n",
    "#     # for debugging, ensure data shuffle works.\n",
    "#     ind = -2\n",
    "#     plt.plot(features_combined[ind][:,0], features_combined[ind][:,1], 'b')\n",
    "#     plt.plot(labels_combined[ind][:,0], labels_combined[ind][:,1], 'r')\n",
    "#     for g in goal_snippets_combined[ind]:\n",
    "#         plt.plot(g[0], g[1], 'kx')\n",
    "#     plt.show()\n",
    "#     plt.plot(features_combined[ind+1][:,0], features_combined[ind+1][:,1], 'b')\n",
    "#     plt.plot(labels_combined[ind+1][:,0], labels_combined[ind+1][:,1], 'r')\n",
    "#     for g in goal_snippets_combined[ind+1]:\n",
    "#         plt.plot(g[0], g[1], 'kx')\n",
    "#     plt.show()\n",
    "N_instances = len(features_combined)\n",
    "dataset_dict = {}\n",
    "dataset_dict['features'] = features_combined\n",
    "dataset_dict['goals']  = goal_snippets_combined\n",
    "dataset_dict['labels'] = labels_combined\n",
    "dataset_dict['N'] = N_instances\n",
    "\n",
    "pickle.dump(dataset_dict, open(dataset_name + '.pkl', 'wb'))\n",
    "\n",
    "if num_folds_cv > 0:\n",
    "    splits = (N_instances // num_folds_cv) * np.ones(num_folds_cv)\n",
    "    splits[:N_instances % num_folds_cv] += 1\n",
    "    \n",
    "    ind_limits = np.cumsum(splits).astype(np.int)\n",
    "    \n",
    "    for i in range(len(ind_limits)):\n",
    "        if i == 0:\n",
    "            ind_start = 0\n",
    "        else:\n",
    "            ind_start = ind_limits[i-1]\n",
    "        ind_end = ind_limits[i]\n",
    "        \n",
    "        dataset_kfold = {}\n",
    "        dataset_kfold['features'] = features_combined[ind_start:ind_end]\n",
    "        dataset_kfold['goals'] = goal_snippets_combined[ind_start:ind_end]\n",
    "        dataset_kfold['labels'] = labels_combined[ind_start:ind_end]\n",
    "        dataset_kfold['N'] = ind_end - ind_start\n",
    "        pickle.dump(dataset_kfold, open(dataset_name + '_fold_%d.pkl' % i, 'wb'))\n",
    "                \n",
    "print('Finished\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
