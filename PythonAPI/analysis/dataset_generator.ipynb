{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes processed pickle files and makes a dataset for prediction.\n",
    "In particular, given a history and prediction horizon, this generates all viable slices of a full trajectory.  This goes through all pickle files and constructs a TFRecord dataset for k-fold cross validation.\n",
    "\n",
    "**Note**: This is just for reference.  Contact Vijay/Xu (emails in the README.md file) if you need access to the raw pickle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from datetime import datetime\n",
    "import sklearn.utils as sku\n",
    "from tqdm import tqdm\n",
    "\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\" \n",
    "\n",
    "from pkl_reader import *\n",
    "from tfrecord_utils import write_tfrecord\n",
    "\n",
    "from utils import get_parking_lot_image_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CONFIG: CONSTANTS FOR EXECUTION '''\n",
    "num_folds_cv = 5 # k-fold cross validation, number of splits (-1 = only provide full set of data)\n",
    "shuffle = True\n",
    "seed = 0\n",
    "prune_start=True          # remove stationary portion of ego's trajectory at the start\n",
    "prune_end=True            # remove stationary portion of ego's trajectory at the end\n",
    "min_vel_thresh=0.01       # velocity threshold (m/s) above which ego is considered moving\n",
    "exclude_collisions=True  # return an empty trajectory if there was a collision\n",
    "\n",
    "Nhist=5          # number of timesteps of motion history to predict with\n",
    "Npred=20         # number of timesteps of prediction horizon\n",
    "Nskip=5          # \"stride\" for sliding window of snippet selection\n",
    "dt=0.1           # discretization (s) of full ego trajectory corresponding to N* above\n",
    "ego_trans = True # whether or not to represent trajectory snippets in the ego frame\n",
    "                 # if False, use the global map frame for all snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ext = 'pkl'\n",
    "file_prefix = '/media/data/carla_parking_data_bkcp/bags/'\n",
    "search_str = file_prefix + '*.' + save_ext \n",
    "files_to_process = glob.glob(search_str)\n",
    "print('Found %d files to read: %s' % (len(files_to_process), files_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_tfrecords = 0\n",
    "skipped_files = 0\n",
    "skipped_users = defaultdict(lambda:0)\n",
    "\n",
    "# full dataset for all files_to_process\n",
    "features_combined = []\n",
    "features_global_combined = []\n",
    "labels_combined = []\n",
    "goal_snpts_combined = []\n",
    "static_objs_combined = []\n",
    "\n",
    "parking_lot = None\n",
    "ego_dims    = None\n",
    "\n",
    "for file in files_to_process:\n",
    "    if save_ext == 'pkl':\n",
    "        res_dict = pickle.load(open(file,'rb'))\n",
    "    else:\n",
    "        raise NotImplemented('Invalid Extension')\n",
    "    \n",
    "    goals = extract_goals(res_dict)\n",
    "    parking_lot = res_dict['parking_lot']\n",
    "    ego_dims = res_dict['ego_dimensions']\n",
    "    \n",
    "    try:\n",
    "        assert goals.shape[0] == 32, \"Invalid goal shape.\"\n",
    "        assert len(res_dict['vehicle_object_lists'][0]) == 56, \"Wrong number of static vehicles.\"\n",
    "        \n",
    "        # parse one demonstration\n",
    "        ego_trajectory, start_ind, switch_ind, end_ind, goal_ind = \\\n",
    "             extract_full_trajectory(res_dict, goals, prune_start, prune_end, \\\n",
    "                                     min_vel_thresh, exclude_collisions)\n",
    "\n",
    "        features, features_global, labels, labels_global, goal_snpts = \\\n",
    "            get_ego_trajectory_prediction_snippets(ego_trajectory, start_ind, switch_ind, end_ind, goal_ind, \\\n",
    "                                           goals, Nhist, Npred, Nskip, dt, ego_frame=ego_trans)\n",
    "        \n",
    "        features_combined.extend(features)\n",
    "        features_global_combined.extend(features_global)\n",
    "        labels_combined.extend(labels)\n",
    "        goal_snpts_combined.extend(goal_snpts)\n",
    "        \n",
    "        static_object_list = res_dict['static_object_list']\n",
    "        for i in range(len(features)):\n",
    "            static_objs_combined.append(static_object_list)\n",
    "    except Exception as e:\n",
    "        print(file, e)\n",
    "        skipped_files += 1\n",
    "        traceback.print_exc()\n",
    "        skipped_users[ file.split('_')[4] ] += 1\n",
    "\n",
    "print(\"Num of skipped files due to exception: \", skipped_files)\n",
    "print(\"Num of skipped files by user: \", skipped_users)\n",
    "        \n",
    "if shuffle:\n",
    "    features_combined, features_global_combined, labels_combined, goal_snpts_combined, static_objs_combined = \\\n",
    "        sku.shuffle(features_combined, features_global_combined, labels_combined, goal_snpts_combined, static_objs_combined, random_state=seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_instances = len(features_combined)\n",
    "    \n",
    "if num_folds_cv > 0:\n",
    "    splits = (N_instances // num_folds_cv) * np.ones(num_folds_cv)\n",
    "    splits[:N_instances % num_folds_cv] += 1\n",
    "    \n",
    "    ind_limits = np.cumsum(splits).astype(np.int)\n",
    "    \n",
    "    for i in range(len(ind_limits)):\n",
    "        if i == 0:\n",
    "            ind_start = 0\n",
    "        else:\n",
    "            ind_start = ind_limits[i-1]\n",
    "        ind_end = ind_limits[i]\n",
    "        \n",
    "        print('Fold', i, ind_start, ind_end)\n",
    " \n",
    "        # write tfrecords in batches from ind_start:ind_end.\n",
    "        #for j in range(0, ind_end - ind_start, batch_size):\n",
    "        #    write data from ind_start + j : min(ind_start + j + batch_size, ind_end)\n",
    "        batch_size = 100 # TODO.\n",
    "        for batch_ind, j in enumerate(range(ind_start,ind_end,batch_size)):\n",
    "            j_min = j\n",
    "            j_max = min(j+batch_size,ind_end)\n",
    "            \n",
    "            print('Batch', batch_ind, j_min, j_max)\n",
    "            \n",
    "            img_hists_batch = np.array(\n",
    "                [get_parking_lot_image_hist(parking_lot, \n",
    "                                            static_objs_combined[k], \n",
    "                                            features_global_combined[k], \n",
    "                                            ego_dims, resize_factor=0.5) for k in range(j_min, j_max)])\n",
    "            \n",
    "            print('Img Hist Shape', img_hists_batch.shape)\n",
    "            \n",
    "            file_location = file_prefix + 'dataset_fold_' + str(i) +'_' + str(batch_ind) + '.tfrecord'\n",
    "            \n",
    "            print('Saving to ', file_location)\n",
    "            \n",
    "            write_tfrecord(features_combined[j_min:j_max],\n",
    "                           img_hists_batch,\n",
    "                           labels_combined[j_min:j_max], \n",
    "                           goal_snpts_combined[j_min:j_max],\n",
    "                           file_location, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For visualization/debugging: see the first snippet of data.\n",
    "for k in range(5):\n",
    "    print('Instance', k)\n",
    "    plt.figure(figsize=(10, 10), dpi=500, facecolor='w', edgecolor='k')\n",
    "    for i in range(Nhist):\n",
    "        plt.subplot(1, Nhist, i+1)\n",
    "        plt.imshow(img_hists_batch[k, i, :, :, :])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "# For debugging: see an arbitrary snippet from the generated tfrecord.\n",
    "image, feature, label, goal, count = read_tfrecord([file_location])\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=160, facecolor='w', edgecolor='k')\n",
    "for i in range(Nhist):\n",
    "    plt.subplot(1, Nhist, i+1)\n",
    "    plt.imshow(image.numpy()[0, i, :, :, :])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hists_batch.shape\n",
    "# img_test = Image.fromarray(img_hists_batch[0, 0, :, :, :])\n",
    "# img_test = img_test.resize((100, 325))\n",
    "# img_test = np.asarray(img_test)\n",
    "\n",
    "# plt.figure(figsize=(10, 10), dpi=100, facecolor='w', edgecolor='k')\n",
    "# plt.axis('equal')\n",
    "# plt.imshow(img_test)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
