# Analysis Code for Parking Prediction.
After running an experiment as detailed in PythonAPI/examples/commands.md with map "exp", we can process the data and train models here.

## Python Prediction Model Implementations and Evaluation:
  * kf_impl.py
    * Extended Kalman Filter with Constant Velocity model.
  * lstm_impl.py
    * Intent/Goal-conditioned LSTM with an intent prediction module and trajectory prediction module.
  * cnn_lstm_impl.py
    * Similar to lstm_impl.py but with additional semantic image image features from a CNN added to the LSTM input.
  * evaluation_metrics.py
    * Contains all the helper functions to compute metrics on predictions from each model and to assemble the k folds for cross-validation.

## Python Dataset Generation Utils:
  * bag_reader.py
    * Contains helper functions to read the rostopics in the rosbag and save a pickle (.pkl) file for each demonstration.
  * pkl_reader.py
    * Reads in a pickle (.pkl) file and generates the dataset snippets used as prediction instances.
  * tfrecord_utils.py
    * Provides writing and parsing functions to save each snippet as an instance in a TFRecord file.
  * transformations.py
    * Helper code written by Christoph Gohlke and similar to the ROS transformations library.  Used for euler_from_quaternion function only.
  * utils.py
    * Misc. utils for semantic birds eye view image generation and trajectory visualization (traj_viz.ipynb).

## Key Notebooks:
  * bag_processing
    * Iterates over all raw rosbag files collected in experiments and provides a simpler pickle (.pkl) file for each demonstration using bag_reader.
  * dataset_generator
    * Iterates over the intermediate pickle (.pkl) files, reading with pkl_reader, and generates snippets that are saved with tfrecord_utils to a set of TFRecord files.
    * We shuffle all data on a snippet level (NOT a demonstration level) and save with a labeled fold id for k-fold dataset generation.
  * models_analysis_tfrecords
    * This is the main file used to train and make predictions on all models (i.e. with suffix impl.py) on the TFRecord dataset generated by dataset_generator.
    * If training, saves files for each model and fold.  If predicting, save pickle files containing all prediction results for each model and fold.
  * models_evaluation_tfrecords
    * This is the file used to compute metrics on the predictions made by models_analysis_tfrecords.
    * Generates metric evaluation plots for visualization.
  * traj_viz
    * Given the trained models (.h5 for LSTM and .pkl for EKF), generates some visualizations.
    * Animates the snippets so that they can be played in a movie form.

## Old/Unused Notebooks (old_notebooks folder):
  * bag_visualization
    * This can help visualize a subset of entries in the bag.  It is out of date with some of the new entries added for the semantic BEV image generation.
  * pkl_visualization
    * Similar to bag_visualization and shows an example of the snippet generation process.  Should work but not fully up to date as with bag_visualization.
  * lstm_many2many
    * Was used to prototype the LSTM trajectory model. Not really useful now.
  * lstm_wip
    * Used to prototype the final LSTM models.  This ended up turning into lstm_impl.py.
  * models_analysis
    * First version of running all models (EKF, LSTM, CNN-LSTM) in a unified testing framework.  Used a pkl file with the snippets - outdated now we use TFRecord.
